{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb950f76-466b-497c-aa3f-d42bee83b993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü§îQualidade dos Dados\n",
    "\n",
    "A an√°lise da qualidade dos dados √© fundamental para garantir a confiabilidade dos resultados obtidos nas consultas e an√°lises de neg√≥cio. Nesta se√ß√£o, ser√£o abordados os principais aspectos relacionados √† integridade, consist√™ncia e completude dos dados presentes nas tabelas do projeto.\n",
    "\n",
    "**Principais pontos a serem avaliados:**\n",
    "- **Valores nulos ou ausentes:** Identificar campos com alta incid√™ncia de valores nulos ou vazios, que podem impactar an√°lises e decis√µes.\n",
    "- **Duplicidade de registros:** Verificar a exist√™ncia de registros duplicados, especialmente em tabelas de transa√ß√µes e clientes.\n",
    "- **Consist√™ncia de tipos de dados:** Garantir que os campos estejam com os tipos de dados corretos (ex: datas, valores num√©ricos, categorias).\n",
    "- **Valida√ß√£o de regras de neg√≥cio:** Conferir se os dados seguem as regras estabelecidas, como formatos de CEP v√°lidos, valores positivos para vendas, etc.\n",
    "- **Referencial entre tabelas:** Avaliar se as chaves estrangeiras est√£o corretamente relacionadas, evitando registros √≥rf√£os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3d6e7c-24f0-4657-9fba-4d760471a233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG `100cep_gateway`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcdb867-1fc6-4030-bd84-bd9311f9d7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE SCHEMA gold;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5221e3b-4815-4413-bc39-6a130a293737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUALIDADE DA TABELA DIM_CLIENTES\n",
    "# 1. Valores nulos cr√≠ticos na tabela dim_clientes\n",
    "spark.sql(\"\"\"SELECT\n",
    "  SUM(CASE WHEN cliente_id IS NULL THEN 1 END) AS nulo_id_cliente,\n",
    "  SUM(CASE WHEN cliente_id_unico IS NULL THEN 1 END) AS nulo_id_cliente_unico\n",
    "FROM dim_clientes;\"\"\").display()\n",
    "\n",
    "# 2. Verificando se h√° duplicidades nos dados\n",
    "spark.sql(\"\"\"SELECT\n",
    "  COUNT(*) AS total,\n",
    "  COUNT(DISTINCT cliente_id) AS total_distinct_id,\n",
    "  COUNT(DISTINCT cliente_id_unico) AS total_distinct_cliente_unico,\n",
    "  COUNT(DISTINCT cep_prefixo) AS total_distinct_cep\n",
    "FROM dim_clientes;\"\"\").display()\n",
    "\n",
    "# 3. Verificando se todos os cep_prefixo possuem 5 n√∫meros\n",
    "spark.sql(\"\"\"SELECT\n",
    "  COUNT(cep_prefixo) AS total_cep_prefixo,\n",
    "  COUNT(DISTINCT CASE WHEN LENGTH(cep_prefixo) != 5 THEN 1 END) AS total_invalidos\n",
    "FROM dim_clientes;\"\"\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d0f49d-564a-4eb3-8ee7-1f61f6221199",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765568579687}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      },
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765569007802}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUALIDADE DA TABELA DIM_DATA\n",
    "# 1. Valores nulos cr√≠ticos na tabela dim_data\n",
    "spark.sql(\"\"\"SELECT\n",
    "  SUM(CASE WHEN data_calendario IS NULL THEN 1 END) AS nulo_data_calendario\n",
    "FROM dim_data;\"\"\").display()\n",
    "\n",
    "# 2. Se todas as datas na dim_data s√£o representadas na fato_transacoes\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  dt.data_calendario,\n",
    "  CASE WHEN ft.data_pedido IS NULL THEN 'N√ÉO REPRESENTADA' ELSE 'REPRESENTADA' END AS status_representacao\n",
    "FROM dim_data AS dt\n",
    "LEFT JOIN fato_transacoes AS ft\n",
    "  ON dt.data_calendario = ft.data_pedido\n",
    "GROUP BY dt.data_calendario, ft.data_pedido;\n",
    "\"\"\").display()\n",
    "\n",
    "# 3. Se todas as datas na fato_transacoes s√£o representadas na dim_data \n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  ft.data_pedido ,\n",
    "  CASE WHEN dt.data_calendario IS NULL THEN 'N√ÉO REPRESENTADA' ELSE 'REPRESENTADA' END AS status_representacao\n",
    "FROM fato_transacoes AS ft\n",
    "LEFT JOIN dim_data AS dt\n",
    "  ON dt.data_calendario = ft.data_pedido\n",
    "GROUP BY ft.data_pedido, dt.data_calendario;\n",
    "\"\"\").display()\n",
    "\n",
    "# 4. Verificando se h√° datas repetidas\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  data_calendario,\n",
    "  COUNT(*) AS qtd\n",
    "FROM dim_data\n",
    "GROUP BY data_calendario\n",
    "HAVING COUNT(*) > 1;\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "612c8813-15db-40f4-be5a-1de31c8b8097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUALIDADE DA TABELA DIM_GEOLOCALIZACAO\n",
    "\n",
    "# 1. Valores nulos cr√≠ticos na tabela dim_geolocalizacao\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN cep_prefixo IS NULL THEN 1 END) AS nulo_cep_prefixo,\n",
    "  SUM(CASE WHEN cidade IS NULL THEN 1 END) AS nulo_cidade,\n",
    "  SUM(CASE WHEN estado IS NULL THEN 1 END) AS nulo_estado,\n",
    "  SUM(CASE WHEN latitude IS NULL THEN 1 END) AS nulo_latitude,\n",
    "  SUM(CASE WHEN longitude IS NULL THEN 1 END) AS nulo_longitude\n",
    "FROM dim_geolocalizacao;\n",
    "\"\"\").display()\n",
    "\n",
    "# 2. Verificando valores distintos na tabela dim_geolocalizacao\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS total,\n",
    "  COUNT(DISTINCT cep_prefixo) AS total_distinct_cep_prefixo,\n",
    "  COUNT(DISTINCT cidade) AS total_distinct_cidade,\n",
    "  COUNT(DISTINCT estado) AS total_distinct_estado\n",
    "FROM dim_geolocalizacao;\n",
    "\"\"\").display()\n",
    "\n",
    "# 3. Valida√ß√£o do formato do cep_prefixo (5 d√≠gitos) na tabela dim_geolocalizacao\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  COUNT(cep_prefixo) AS total_cep_prefixo,\n",
    "  COUNT(DISTINCT CASE WHEN LENGTH(cep_prefixo) != 5 THEN cep_prefixo END) AS total_invalidos\n",
    "FROM dim_geolocalizacao;\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb03b1c-766f-4d0b-9c7f-46ffc7ed5f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUALIDADE DA TABELA DIM_PAGAMENTOS\n",
    "\n",
    "# 1. Valores nulos cr√≠ticos na tabela dim_pagamentos\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN id_pagamento IS NULL THEN 1 END) AS nulo_id_pagamento,\n",
    "  SUM(CASE WHEN tipo_pagamento IS NULL THEN 1 END) AS nulo_tipo_pagamento,\n",
    "  SUM(CASE WHEN nivel_risco IS NULL THEN 1 END) AS nulo_nivel_risco\n",
    "FROM dim_pagamentos;\n",
    "\"\"\").display()\n",
    "\n",
    "# 2. Verificando valores distintos na tabela dim_pagamentos\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS total,\n",
    "  COUNT(DISTINCT id_pagamento) AS total_distinct_id_pagamento,\n",
    "  COUNT(DISTINCT tipo_pagamento) AS total_distinct_tipo_pagamento,\n",
    "  COUNT(DISTINCT nivel_risco) AS total_distinct_nivel_risco\n",
    "FROM dim_pagamentos;\n",
    "\"\"\").display()\n",
    "\n",
    "# 3. Verificando duplicidade de id_pagamento\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  id_pagamento,\n",
    "  COUNT(*) AS qtd\n",
    "FROM dim_pagamentos\n",
    "GROUP BY id_pagamento\n",
    "HAVING COUNT(*) > 1;\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92713c72-9e04-4bdf-b76c-3a04137fbfe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# QUALIDADE DA TABELA DIM_VENDEDORES\n",
    "\n",
    "# 1. Valores nulos cr√≠ticos na tabela dim_vendedores\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN vendedor_id IS NULL THEN 1 END) AS nulo_id_vendedor,\n",
    "  SUM(CASE WHEN cep_prefixo IS NULL THEN 1 END) AS nulo_cep_prefixo\n",
    "FROM\n",
    "  dim_vendedores;\"\"\").display()\n",
    "\n",
    "# 2. Valida√ß√£o do formato do cep_prefixo (5 d√≠gitos) na tabela dim_vendedores\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  COUNT(cep_prefixo) AS total_cep_prefixo,\n",
    "  COUNT(DISTINCT CASE WHEN LENGTH(cep_prefixo) != 5 THEN cep_prefixo END) AS total_invalidos\n",
    "FROM dim_vendedores;\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b27e0176-5503-4568-8270-9cbd45b7ce35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "# QUALIDADE DA TABELA \n",
    "-- Valores nulos cr√≠ticos na tabela fato_transacoes\n",
    "SELECT\n",
    "  SUM(CASE WHEN cliente_id IS NULL THEN 1 END) AS nulo_id_cliente,\n",
    "  SUM(CASE WHEN pedido_id IS NULL THEN 1 END) AS nulo_id_pedido,\n",
    "  SUM(CASE WHEN vendedor_id IS NULL THEN 1 END) AS nulo_id_vendedor\n",
    "FROM fato_transacoes;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5335954409743059,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06_analise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
